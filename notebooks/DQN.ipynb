{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import torchvision.transforms as T\n",
    "\n",
    "env = gym.make('CarRacing-v0')\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.image as mpimg\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DQN(nn.Module): \n",
    "    def __init__(self, outputs):\n",
    "        super(DQN, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 8, kernel_size = 7, stride = 3)\n",
    "        self.bn1 = nn.BatchNorm2d(8)\n",
    "        self.conv2 = nn.Conv2d(8, 32, kernel_size = 3)\n",
    "        self.bn2 = nn.BatchNorm2d(32)\n",
    "        self.fc1 = nn.Linear(32 * 3 * 3, 256)\n",
    "        self.fc2 = nn.Linear(256, outputs)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.bn1(self.conv1(x)))\n",
    "        x = F.max_pool2d(x)\n",
    "        x = F.relu(self.bn2(self.conv2(x)))\n",
    "        x = F.max_pool2d(x)\n",
    "        # Flatten the input\n",
    "        x = x.view(-1, self.num_flat_features(x))\n",
    "        x = F.relu(self.fc1(x))\n",
    "        # Softmax activation for the last layer         \n",
    "        x = F.softmax(self.fc2(x))\n",
    "        \n",
    "        return x\n",
    "    \n",
    "    def num_flat_features(self, x):\n",
    "        size = x.size()[1:]   # all dimensions except the batch dimension\n",
    "        num_features = 1\n",
    "        for s in size:\n",
    "            num_features *= s\n",
    "        return num_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DQN(\n",
      "  (conv1): Conv2d(1, 8, kernel_size=(7, 7), stride=(3, 3))\n",
      "  (bn1): BatchNorm2d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (conv2): Conv2d(8, 32, kernel_size=(3, 3), stride=(1, 1))\n",
      "  (bn2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (fc1): Linear(in_features=288, out_features=256, bias=True)\n",
      "  (fc2): Linear(in_features=256, out_features=10, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "model = DQN(10)\n",
    "\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import namedtuple, deque\n",
    "\n",
    "class History:\n",
    "    def __init__(self, capacity, batch_size, seed):\n",
    "        self.capacity = capacity\n",
    "        self.memory = deque(maxlen = capacity)\n",
    "        self.transition = namedtuple('Transition', ('state', 'action', 'reward', 'next_state', 'done'))\n",
    "        self.batch_size = batch_size\n",
    "        self.seed = random.seed(seed)\n",
    "\n",
    "    def push(self, state, action, reward, next_state, done):\n",
    "        \"\"\"Saves a transition.\"\"\"\n",
    "        t = self.transition(state, action, reward, next_state, done)\n",
    "        self.memory.append(e)\n",
    "\n",
    "    def sample(self, batch_size):\n",
    "        transitions = random.sample(self.memory, self.batch_size)\n",
    "        \n",
    "        states = torch.from_numpy(np.vstack([e.state for e in experiences if e is not None])).float().to(device)\n",
    "        actions = torch.from_numpy(np.vstack([e.action for e in experiences if e is not None])).long().to(device)\n",
    "        rewards = torch.from_numpy(np.vstack([e.reward for e in experiences if e is not None])).float().to(device)\n",
    "        next_states = torch.from_numpy(np.vstack([e.next_state for e in experiences if e is not None])).float().to(device)\n",
    "        dones = torch.from_numpy(np.vstack([e.done for e in experiences if e is not None]).astype(np.uint8)).float().to(device)\n",
    "        \n",
    "        return (states, actions, reqrds, next_states, dones)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.memory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "'break' outside loop (cell_name, line 154)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"cell_name\"\u001b[0;36m, line \u001b[0;32m154\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m 'break' outside loop\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import itertools as it\n",
    "import random\n",
    "from skimage import color, transform\n",
    "from collections import namedtuple, deque\n",
    "from model import DQN\n",
    "from experience_history import History\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "class DQNAgent:\n",
    "    def __init__(self, env):\n",
    "        self.env = env\n",
    "        self.global_counter = 0\n",
    "\n",
    "        # Training Parameters\n",
    "        self.batch_size = 64\n",
    "        self.image_size = (96, 96)\n",
    "        self.gamma = 0.95 \n",
    "        self.initial_epsilon = 1.0\n",
    "        self.min_epsilon = 0.1\n",
    "        self.epsilon_decay_steps = int(1e6)\n",
    "        self.learning_rate = 4e-4\n",
    "        self.tau = 1e-3\n",
    "        self.network_update_frequency = 4\n",
    "\n",
    "        # Enviroment\n",
    "        self.render = True\n",
    "        self.seed = 7    # Seed to random \n",
    "\n",
    "        # Possible Actions and their corresponding weights\n",
    "        left_right = [-1, 0, 1]\n",
    "        acceleration = [1, 0]\n",
    "        brake = [0.3, 0]\n",
    "        all_actions = np.array([action for action in it.product(left_right, acceleration, brake)])\n",
    "        self.action_map = all_actions\n",
    "        self.num_actions = len(self.action_map)\n",
    "        gas_actions = [a[1] == 1 and a[2] == 0 for a in self.action_map]\n",
    "        # Increase the weight of gas actions for the car.\n",
    "        self.action_weights = 14 * gas_actions + 1\n",
    "        self.action_weights /= np.sum(self.action_weights)\n",
    "\n",
    "\n",
    "        # Model (Neural Network)\n",
    "        self.training_model = DQN(self.num_actions)\n",
    "        self.target_model = DQN(self.num_actions)\n",
    "        self.optimizer = optim.Adam(self.training_model.parameters(), lr = self.learning_rate)\n",
    "\n",
    "        # Negative Reward\n",
    "        # To check if we want to end the episode earlier\n",
    "        self.neg_reward_counter = 0\n",
    "        self.max_neg_rewards = 100\n",
    "\n",
    "        # History\n",
    "        self.experience_capacity = int(1e5)\n",
    "        self.memory = History(self.experience_capacity, self.batch_size, self.seed)\n",
    "\n",
    "    def step(self, state, action, reward, next_state, done):\n",
    "        # Save experience in replay memory\n",
    "        self.memory.add(state, action, reward, next_state, done)\n",
    "\n",
    "        # Learn every UPDATE_EVERY time steps.\n",
    "        self.t_step = (self.t_step + 1) % self.network_update_frequency\n",
    "        if self.t_step == 0:\n",
    "            # If enough samples are available in memory, get random subset and learn\n",
    "            if len(self.memory) > self.batch_size:\n",
    "                experiences = self.memory.sample()\n",
    "                self.learn(experiences)\n",
    "\n",
    "    def get_action(self, state):\n",
    "        \"\"\"Returns actions for given state as per current policy.\n",
    "\n",
    "        Params\n",
    "        ======\n",
    "            state (array_like): current state\n",
    "            eps (float): epsilon, for epsilon-greedy action selection\n",
    "        \"\"\"\n",
    "        state = torch.from_numpy(state).float().unsqueeze(0).to(device)\n",
    "        self.training_model.eval()\n",
    "        with torch.no_grad():\n",
    "            action_values = self.training_model(state)\n",
    "\n",
    "        self.training_model.train()\n",
    "\n",
    "        # Epsilon-greedy action selection\n",
    "        if random.random() > self.get_epsilon():\n",
    "            return np.argmax(action_values.cpu().data.numpy())\n",
    "        else:\n",
    "            return self.get_random_action()\n",
    "\n",
    "    def learn(self, experiences):\n",
    "        \"\"\"Update value parameters using given batch of experience tuples.\n",
    "\n",
    "        Params\n",
    "        ======\n",
    "            experiences (Tuple[torch.Tensor]): tuple of (s, a, r, s', done) tuples \n",
    "            gamma (float): discount factor\n",
    "        \"\"\"\n",
    "        states, actions, rewards, next_states, dones = experiences\n",
    "\n",
    "        # Get max predicted Q values (for next states) from target model\n",
    "        Q_targets_next = self.target_model(next_states).detach().max(1)[0].unsqueeze(1)\n",
    "        # Compute Q targets for current states \n",
    "        Q_targets = rewards + (self.gamma * Q_targets_next * (1 - dones))\n",
    "\n",
    "        # Get expected Q values from local model\n",
    "        Q_expected = self.training_model(states).gather(1, actions)\n",
    "\n",
    "        # Compute loss\n",
    "        loss = F.mse_loss(Q_expected, Q_targets)\n",
    "        # Minimize the loss\n",
    "        self.optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        self.optimizer.step()\n",
    "\n",
    "        # ------------------- update target network ------------------- #\n",
    "        self.soft_update(self.training_model, self.target_model)                     \n",
    "\n",
    "    def soft_update(self, local_model, target_model):\n",
    "        \"\"\"Soft update model parameters.\n",
    "        θ_target = τ*θ_local + (1 - τ)*θ_target\n",
    "\n",
    "        Params\n",
    "        ======\n",
    "            local_model (PyTorch model): weights will be copied from\n",
    "            target_model (PyTorch model): weights will be copied to\n",
    "            tau (float): interpolation parameter \n",
    "        \"\"\"\n",
    "        for target_param, local_param in zip(target_model.parameters(), local_model.parameters()):\n",
    "            target_param.data.copy_(self.tau * local_param.data + (1.0 - self.tau) * target_param.data)\n",
    "          \n",
    "    def get_epsilon(self):\n",
    "        if self.global_counter >= self.epsilon_decay_steps:\n",
    "            return self.min_epsilon\n",
    "        else:\n",
    "            # linear decay\n",
    "            r = 1.0 - self.global_counter / float(self.epsilon_decay_steps)\n",
    "            return self.min_epsilon + (self.initial_epsilon - self.min_epsilon) * r\n",
    "          \n",
    "    def play_episode(self):\n",
    "        state = self.env.reset()\n",
    "        score = 0\n",
    "        scores = []\n",
    "        self.global_counter = 0\n",
    "        while True:\n",
    "            self.global_counter += 1\n",
    "            action = self.get_action(state)\n",
    "            next_state, reward, done, _ = self.env.step(action)\n",
    "            self.step(state, action, reward, next_state, done)\n",
    "            state = next_state\n",
    "            score += reward\n",
    "            if done:\n",
    "                break \n",
    "\n",
    "        scores.append(score)              # save most recent score\n",
    "        return \n",
    "    \n",
    "  \n",
    "  # Convert RGB Image to grayscale and de\n",
    "    def process_image(self, img):\n",
    "        return 2 * color.rgb2gray(img) - 1\n",
    "\n",
    "  # Returns a random action.\n",
    "    def get_random_action(self):\n",
    "        return np.random.choice(self.num_actions, p=self.action_weights)\n",
    "  \n",
    "    def check_early_stop(self, reward, total_reward):\n",
    "    if reward < 0:\n",
    "        self.neg_reward_counter += 1\n",
    "        done = (self.neg_reward_counter > self.max_neg_rewards)\n",
    "\n",
    "        if done and total_reward <= 500:\n",
    "            punishment = -20.0\n",
    "        else:\n",
    "            punishment = 0.0\n",
    "\n",
    "        if done:\n",
    "            self.neg_reward_counter = 0\n",
    "\n",
    "        return done, punishment\n",
    "    else:\n",
    "        self.neg_reward_counter = 0\n",
    "        return False, 0.0\n",
    "\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Track generation: 1199..1503 -> 304-tiles track\n"
     ]
    }
   ],
   "source": [
    "env.reset()\n",
    "\n",
    "for _ in range(100):\n",
    "    env.render()\n",
    "    observation, reward, done, info = env.step(env.action_space.sample()) # take a random action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from skimage import color, transform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "observation = 2 * color.rgb2gray(observation) - 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(96, 96)\n",
      "(1, 96, 96)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[[ 0.37232   ,  0.37232   ,  0.37232   , ...,  0.37232   ,\n",
       "          0.37232   ,  0.37232   ],\n",
       "        [-0.2       , -0.2       ,  0.37232   , ...,  0.37232   ,\n",
       "          0.37232   ,  0.37232   ],\n",
       "        [-0.2       , -0.2       , -0.16078431, ...,  0.37232   ,\n",
       "          0.37232   ,  0.37232   ],\n",
       "        ...,\n",
       "        [-1.        , -1.        , -1.        , ..., -1.        ,\n",
       "         -1.        , -1.        ],\n",
       "        [-1.        , -1.        , -1.        , ..., -1.        ,\n",
       "         -1.        , -1.        ],\n",
       "        [-1.        , -1.        , -1.        , ..., -1.        ,\n",
       "         -1.        , -1.        ]]])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(observation.shape)\n",
    "\n",
    "new_ob = observation[np.newaxis, ...]\n",
    "print(new_ob.shape)\n",
    "new_ob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(b_w_image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transformed_image = transform.rescale(observation, 0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(transformed_image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img = process_image(observation)\n",
    "img.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(img, cmap=plt.cm.gray)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "observation[]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = [[[1,2],[3,4]],[[5,6],[7,8]]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a[1:7]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "-np.ones(100, dtype=\"int32\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "int(1e5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.action_space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools as it\n",
    "all_actions = np.array([k for k in it.product([-1, 0, 1], [1, 0], [0.2, 0])])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "gas_actions = np.array([a[1] == 1 and a[2] == 0 for a in all_actions])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([False,  True, False, False, False,  True, False, False, False,\n",
       "        True, False, False])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gas_actions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 1. 15.  1.  1.  1. 15.  1.  1.  1. 15.  1.  1.]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "9"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "action_weights = 14.0 * gas_actions + 1.0\n",
    "print(action_weights)\n",
    "action_weights /= np.sum(action_weights)\n",
    "np.random.choice(len(all_actions), p=action_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "action_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "possible_actions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#     def __init__(self):\n",
    "#         self.capacity = int(1e5)\n",
    "#         self.image_size = (96, 96)\n",
    "        \n",
    "#         self.mem_idx = 0\n",
    "#         self.num_frame_in_stack = 4\n",
    "#         self.sliding_window = None\n",
    "#         self.expecting_new_episode = True\n",
    "        \n",
    "#         self.max_frame_cache = self.capacity + 2 * self.num_frame_in_stack + 1\n",
    "#         self frames = -np.ones((self.max_frame_cache,) + self.image_size, dtype = 'float32')\n",
    "        \n",
    "#         self.memory = []\n",
    "        \n",
    "#     def push(self, frame, action, done, reward):\n",
    "#         if len(self.memory) < self.capacity:\n",
    "#             self.memory.append(None)\n",
    "        \n",
    "#         self.state =\n",
    "#         self.memory[self.mem_idx] = Transition()\n",
    "#         # Allocate memory"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:deeplearning]",
   "language": "python",
   "name": "conda-env-deeplearning-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
